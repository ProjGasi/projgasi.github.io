<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Articles — Project GASI (Global AI Security Initiative) – Protecting AI from Modern Threats</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet">
<link rel="stylesheet" href="style.css">


<meta name="description" content="Explore articles, research updates, and insights from the Collaborative AI Red Team project - Global AI Security Initiative.">
  <meta name="keywords" content="AI security, artificial intelligence risks, AI attacks, cybersecurity, AI vulnerabilities, global initiative">

  <!-- Open Graph -->
  <meta property="og:title" content="Articles — Project GASI (Global AI Security Initiative)">
  <meta property="og:description" content="Explore articles, research updates, and insights from the Project GASI (Global AI Security Initiative).">
  <meta property="og:image" content="https://projgasi.github.io/images/preview.jpg">
  <meta property="og:url" content="https://projgasi.github.io/articles.html">
  <meta property="og:type" content="website">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Articles — Project GASI (Global AI Security Initiative)">
  <meta name="twitter:description" content="Protecting AI from real-world threats.">
  <meta name="twitter:image" content="https://projgasi.github.io/images/preview.jpg">

  <!-- Canonical URL -->
  <link rel="canonical" href="https://projgasi.github.io/articles.html">

  <!-- Favicon -->
  <link rel="icon" href="https://projgasi.github.io/images/favicon.ico" type="image/x-icon">

  <!-- Structured data (JSON-LD) -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "WebPage",
    "name": "Articles — Project GASI (Global AI Security Initiative)",
    "url": "https://projgasi.github.io/articles.html",
    "description": "Explore articles, research updates, and insights from the Project GASI (Global AI Security Initiative).",
    "publisher": {
      "@type": "Organization",
      "name": "Project GASI (Global AI Security Initiative)"
    }
  }
  </script>


















</head>
<body>

<header class="header-fixed">
  <div class="header-container">
    
    <div class="logo-container">
      <img src="images/logo.png" alt="Logo" style="height:50px;">
    </div>

    
    <nav>
      <ul>
        <li><a href="/index.html#main" title="Go to main section">MAIN</a></li>
<li><a href="https://projgasi.gt.tc" title="Project GASI – Test">TEST SYSTEM</a></li>
<li><a href="/index.html#roadmap" title="View the project roadmap">ROADMAP</a></li>
<li><a href="/index.html#examples" title="See real examples of AI attacks">EXAMPLES</a></li>
<li><a href="/index.html#news" title="News and updates">NEWS</a></li>
<li><a href="#articles" title="Articles">ARTICLES</a></li>
<li><a href="/index.html#contact" title="Contact the project team">CONTACT</a></li>
      </ul>
    </nav>

    
    <div class="header-cta">
      <a class="cta" href="/index.html#support" id="support-btn">Support Project</a>
    </div>
  </div>
</header>

  
  <section class="fund-roadmap" id="articles">
    <h2 class="inter-large">Articles</h2>
    <p class="lead">Insights, research updates, and development notes</p>

    <div class="article-toc">
  
  <ul>
<li><a href="#rg">About the Graph Reasoners Project</a></li>
<li><a href="#reasoning-graph-article">We Are Building Our Own Reasoning Graph and Reasoner</a></li>
<li><a href="#llm-security-launch">We Have Launched an Early Test System for LLM Security Analysis</a></li>
<li><a href="#ai-human-labor">AI and Human Labor: How to Adapt to the New Reality</a></li>
<li><a href="#ai-open-weights">The Dangers of Open Weights: A New Wave of Risks</a></li>
<li><a href="#ai-security-support">We Have Refined Our Approach to Supporting the Project</a></li>
 <li><a href="#ai-moratorium-safety">In Support of a Moratorium on Superintelligence — Until We Can Reliably Control It</a></li>   
<li><a href="#collective-reasoning-models">Collective reasoning of models with different thinking styles — a step toward human-like idea discussion</a></li>
<li><a href="#ai-stuck-in-time">AI gets stuck in time</a></li>
    <li><a href="#university-lectures">University Lectures as a New Source for Safe AI Training</a></li>
    
  </ul>
</div>








<div class="grid-section">

      <!-- Article 1 -->




<!-- Article -->

<div class="card" id="rg">

  <p class="inter-medium">
    About the Graph Reasoners Project
  </p><br>



  <p class="inter-small">
    As part of our work, we have developed two graph-based reasoners that address different but conceptually related problems.
  </p><br>

  <p class="inter-small"><b>AI Safety Reasoner</b></p>

  <p class="inter-small">
    The first reasoner is being developed as part of an AI safety project Dailogix. Its primary role is to assess the risk and potential danger of user prompts and model responses. It is intended to be integrated into our red teaming system Dailogix, where it will be used for automated analysis of potentially harmful, risky, or otherwise unacceptable reasoning chains.
  </p><br>

  <p class="inter-small">
    This reasoner is designed to identify hidden causal relationships, dangerous combinations of facts, actions, and states — patterns that are difficult to detect using simple heuristics or conventional filtering approaches.
  </p><br>

  <p class="inter-small"><b>General-Purpose Reasoner</b></p>

  <p class="inter-small">
    The second reasoner is a standalone research project aimed at building thinking AI systems as an alternative to the currently dominant transformer-based, probabilistic text generation paradigm.
  </p><br>

  <p class="inter-small">
    Within this project, we have developed a universal graph structure composed of nodes and edges that allows us to represent:
  </p>

  <p class="inter-small">
    – the structure of the world,<br>
    – processes occurring within it,<br>
    – physical and logical laws,<br>
    – facts and states,<br>
    – actions and their consequences,<br>
    – abstract and applied knowledge.
  </p><br>

  <p class="inter-small">
    The core idea is that reasoning can be implemented as the propagation of influence through a knowledge graph, rather than as the generation of token sequences.
  </p><br>

  <p class="inter-small"><b>Current State of the Project</b></p>

  <p class="inter-small">
    At this stage, a test prototype of the reasoner has been implemented, and a graph with a small, partially populated set of meaningful nodes and relations has been created. An example of the reasoner’s operation can be seen in the accompanying video.
  </p><br>
<video
  autoplay
  loop
controls
  muted
  playsinline
  style="max-width:100%; border-radius:8px;">
  <source src="video/rg.mp4" type="video/mp4">
</video>
<br>
<br>
  <p class="inter-small">
    In addition to the meaningful test content, the graph also contains 10 million nodes with random content. These nodes are used solely for performance and scalability evaluation, not for logical inference or reasoning.
  </p><br>

  <p class="inter-small"><b>Graph Training</b></p>

  <p class="inter-small">
    One of the main open research questions is how to train the graph effectively. We are currently running multiple experiments, and so far the best results have been achieved using large language models (LLMs) as a source of structured knowledge during the training process.
  </p><br>

  <p class="inter-small">
    Unfortunately, our current computational resources do not allow us to perform training at high speed, as the inference speed of LLMs remains relatively low. We hope to acquire or rent more powerful servers equipped with AI accelerators in the near future to significantly speed up this phase.
  </p><br>

  <p class="inter-small"><b>Differences from LLMs</b></p>

  <p class="inter-small">
    The video also highlights an important property of the reasoner: its outputs are fully deterministic. Given the same inputs and conditions, it always produces the same result, in contrast to LLMs, whose responses may vary between runs.
  </p><br>

  <p class="inter-small">
    At the same time, the current prototype does not include a sophisticated language layer, so interaction with it takes place using a simplified and constrained vocabulary. This is a deliberate limitation of the current development stage.
  </p><br>

  <p class="inter-small">
    Published: February 02, 2026
  </p>

</div>











<div class="card" id="reasoning-graph-article">
  <p class="inter-medium">
    Why We Are Building Our Own Reasoning Graph and Reasoner for Red Teaming Instead of Using Open-Source Solutions
  </p><br>




<div style="display:flex; gap:10px; margin:10px 0;">





    
    <img src="images/reason_graph.jpg" alt="Reazonex" style="width:90%;">
</div>
  <p class="inter-small">
    As part of the development of our Red Team system, we face a critical task: automatically analyzing user queries and model responses for dangerous, harmful, or otherwise unacceptable content. At the moment, our test environment relies on heuristics — they are simple and fast, but limited in scope, struggle with complex harmful chains, and do not scale well.
  </p><br>

  <p class="inter-small">
    Early in the project, we explored alternative approaches, including the use of compact LLMs for classifying queries and responses. After testing, we found that accuracy was unstable and performance was too slow for a system that must operate in real time and evaluate every model interaction.
  </p><br>

  <p class="inter-small">
    As a result, we made a strategic decision: to build our own reasoning graph architecture and our own reasoner, fully tailored to the tasks of AI safety and red teaming. We decided to name this project Reazonex. 
  </p><br>
<img src="images/reazonex_logo.png" alt="Reazonex" style="width:33%;"><br>
<p class="inter-small">
Below are the key reasons why we chose to develop our own system rather than rely on existing open-source solutions.
</p><br>

  <p class="inter-small"><b>1. Transparency and explainability — a fundamental requirement for AI safety</b></p>
  <p class="inter-small">
    Most open-source reasoners operate as black boxes. They hide internal inference logic, rely on complex and opaque rules, and cannot clearly justify how a particular conclusion was reached. For AI safety, where explainability is essential, this is insufficient.
  </p>
  <p class="inter-small">
    A custom reasoning graph allows us to explicitly control node types, define the semantics of each relationship, trace dangerous chains from source to intent, and clearly justify why a query was classified as unsafe. This strengthens auditability and supports future certification of safety-critical systems.
  </p><br>

  <p class="inter-small"><b>2. Open-source reasoners are not designed for harmful chain analysis</b></p>
  <p class="inter-small">
    Existing graph engines and knowledge-reasoning frameworks were built for tasks such as semantic search, ontologies, recommendations, and information retrieval. None of them can analyze chains like:
    <i>object → action → target → intent → outcome</i>
    and determine when the combination becomes dangerous.
  </p>
  <p class="inter-small">
    Red Teaming requires unique capabilities: context-dependent action analysis, sequential reasoning, intent evaluation, harmful-chain detection, and deterministic rules for blocking unsafe behavior. Open-source solutions do not provide this.
  </p><br>

  <p class="inter-small"><b>3. Open-source licenses introduce legal risks for commercial products</b></p>
  <p class="inter-small">
    Many ready-made reasoning systems use licenses such as GPL/AGPL, which forbid integration into closed commercial products. Apache 2.0 introduces patent obligations. MIT/BSD are safer, but many dependent models and datasets come with restrictions.
  </p>
  <p class="inter-small">
    For Red Team and AI safety deployments — especially in corporate and governmental environments — legal clarity is a critical requirement. Building our own engine eliminates license risks entirely.
  </p><br>

  <p class="inter-small"><b>4. Our own architecture gives us deterministic behavior — something LLMs and open-source reasoners cannot guarantee</b></p>
  <p class="inter-small">
    Open-source reasoners often depend on probabilistic methods or embeddings that may produce inconsistent or unpredictable outputs. For security applications, this is unacceptable: the system must always return the same verdict for the same input.
  </p>
  <p class="inter-small">
    Our proprietary engine is fully deterministic, reproducible, and controllable — a foundational requirement for safety-critical logic.
  </p><br>

  <p class="inter-small"><b>5. Guaranteed performance: reasoning graphs are thousands of times faster than LLMs</b></p>
  <p class="inter-small">
    Even compact LLMs take 20–200 ms locally and 100–600 ms via API to classify a single message. A graph-based reasoner runs in 0.2–2 ms, performs only a few SQL queries, and reliably handles every model prompt in real time.
  </p>
  <p class="inter-small">
    In Red Teaming, speed is essential — the system cannot wait for an LLM to "think".
  </p><br>

  <p class="inter-small"><b>6. This architecture may evolve into a standalone product — not just a security tool</b></p>
  <p class="inter-small">
    We are starting with a simple implementation built with PHP and MySQL. It can be embedded into any modern website, requires no external AI services, and runs even on low-cost shared hosting.
  </p>
  <p class="inter-small">
    Many researchers consider structured reasoning graphs the likely next step in AI evolution beyond transformers. What begins today as a safety module may eventually grow into a standalone reasoning engine, an Explainable-AI component, a logical inference system, or even a commercial product in its own right.
  </p><br>



  <p class="inter-small"><b>Conclusion</b></p>
  <p class="inter-small">
    We have chosen to build our own reasoning graph and reasoner because this approach best supports the requirements of AI safety: transparency, determinism, legal clarity, performance, and precision. At the same time, the technology we are creating has the potential to evolve far beyond safety applications and become a foundation for next-generation reasoning AI.
  </p>
  <br>

Published: December 09, 2025
</div>









<div class="card" id="llm-security-launch">
  <p class="inter-medium">We Have Launched Dailogix -  an Early Test System for LLM Security Analysis</p><br>

  

  <p class="inter-small">
    We are introducing Dailogix an early test version of our system designed to analyze the security of large language models (LLMs) and enable users to exchange up-to-date prompts that expose model vulnerabilities. Despite having a minimal feature set, the prototype already allows for identifying dangerous queries, evaluating model behavior, and detecting weak points in LLM responses.
  </p><br>

  <p class="inter-small">
    At this stage, the system <b>does not include the client-side component</b> that will later automatically detect suspicious or harmful prompts and responses, and block them when necessary. Instead, in the test prototype, staff maintaining the LLM can <b>manually run checks</b>, create basic configurations, and tailor the system to their specific AI usage needs.
  </p><br>

  <p class="inter-small">
    The system evaluates how dangerous a prompt is and determines how the LLM responds: whether the model is willing to help, whether it reports that such queries are not allowed, or whether it takes a neutral stance. Currently, these assessments are based on <b>simple heuristic rules</b>, designed to identify four categories of dangerous topics: Biohazard, Drugs, Explosives, Hacking.
  </p><br>

  <p class="inter-small">
    In the future, we plan to integrate a specially trained AI model that will generate prompts for stress-testing LLMs and evaluate model responses with greater accuracy and contextual understanding.
  </p><br>

  <p class="inter-medium">How to Use the Test Prototype</p><br>

  <p class="inter-small">
    <b>1. Follow the link:</b><a href="https://projgasi.gt.tc" target="_blank">projgasi.gt.tc</a><br><br>
    <b>2. Register</b> to access the dashboard.<br><br>
<img src="images/llm-security.png" alt="LLM security testing system" style="width:100%; margin:10px 0;"><br>
    <b>3. Configure parameters before running a test:</b><br>
    - API URL — the address of your model’s server or API endpoint.<br>
    - API format — you can specify your own format or use predefined REST API and OpenAI-style templates.<br>
    - API token — if required for access.<br>
    - Tags and Secrets — fields for storing keywords that can be dynamically inserted into prompts.<br>
    - Prompt Templates — includes prompts with parameters from Tags and Secrets or static prompts.<br>
    - Forbidden Words — words that the LLM must never produce under any circumstances.<br><br>
    The <b>Default</b> button fills the Prompt Templates and Forbidden Words with standard values. After making any changes, press <b>Save</b>.
  </p><br>

  <p class="inter-small">
    Once the settings are configured, press the run button at the top of the dashboard.  
    Prompts that lead to incorrect responses or the appearance of forbidden words appear in the <b>Alerts</b> section.  
    Prompts that reveal a model vulnerability are added to the <b>global prompt list</b> and used in tests for all other users, enabling the community to collaboratively improve AI safety.
 
<a href="https://www.youtube.com/watch?v=8XoFa6vVZ54" target="_blank">Dailogix on Youtube</a>
<br>
For any questions, technical issues, or bugs you discover, please contact us: <a href="mailto:projgasi@proton.me">projgasi@proton.me</a>
</p><br>


Published: November 27, 2025
</div>











<div class="card" id="ai-human-labor">
  <p class="inter-medium">AI and Human Labor: How to Adapt to the New Reality</p><br>
  <img src="images/ai-labor.jpg" alt="AI and human labor adaptation" style="width:100%; margin:10px 0;"><br>

  <p class="inter-small">
    The challenges brought by the development of artificial intelligence (AI) can be broadly divided into two categories.
    The first concerns AI safety — the risks of errors, misuse, and threats posed by autonomous systems.
    The second relates to societal fears, especially among ordinary people who may lose their jobs as automation and AI systems replace human labor.
  </p><br>

  <p class="inter-small">
    We will discuss safety issues later within our project, but for now, let’s focus on the risks associated with the replacement of human workers in the labor market.
    Before the rise of AI, most people worked full-time — often to the point of exhaustion — and some even held multiple jobs simultaneously.
    As AI becomes more capable and widespread, the amount of human labor required to perform the same tasks decreases.
    Of course, there will always be demand for human-centered activities — personal services, sports, creative hobbies, and handmade production.
    However, the overall amount of labor necessary to sustain human life and development is steadily declining as automation advances.
  </p><br>

  <p class="inter-small">
    If people continue to follow the familiar principle of “working until exhaustion,” there may simply not be enough paid work for everyone.
    There are three possible scenarios for how this situation might evolve.
  </p><br>

  <p class="inter-small"><b>Scenario 1 — “Wild Automation”</b><br>
    (No regulation or social adaptation)<br>
    Governments fail to regulate AI adoption.
    Companies rapidly implement AI, replacing human workers to cut costs and gain short-term superprofits.
    AI developers also accumulate enormous wealth.
  </p><br>

  <p class="inter-small">
    Unemployment benefits shrink, taxes rise, and more people are forced to return to subsistence farming, barter, or small-scale production.
    Demand for cars, electronics, and advanced consumer goods collapses.
    The majority of the population, lacking sufficient income, loses access to the internet, healthcare, and education.
    An economic crisis unfolds, followed by technological decline.
    Countries exporting food or raw materials may retain minor advantages, but global degradation becomes inevitable.
  </p><br>

  <p class="inter-small"><b>Scenario 2 — “Regulation Through Taxation and Redistribution”</b><br>
    (The state reacts but does not change the labor model)<br>
    As AI adoption grows, governments raise taxes on companies to finance unemployment benefits.
    However, economic efficiency drops — companies lose motivation to innovate as excess profits are absorbed by taxation.
  </p><br>

  <p class="inter-small">
    People living on welfare become bored and disengaged, losing interest in creativity, work, and self-development.
    Demand for cheap entertainment rises.
    Moral and cultural stagnation sets in; society becomes passive and dependent on state support.
    Demand for nonessential goods falls, the economy stagnates, and social tensions grow.
    While the system may appear stable on the surface, in the long run, it decays from within.
  </p><br>

  <p class="inter-small"><b>Scenario 3 — “Smart Equilibrium”</b><br>
    (A transition toward a new culture of labor and life)<br>
    Governments, businesses, and society collectively realize that redistributing both wealth and labor is inevitable — no one can “pull the blanket over themselves” without triggering global consequences.
    Businesses stop chasing immediate superprofits and begin introducing AI in ways that reduce overall workloads and improve quality of life.
  </p><br>

  <p class="inter-small">
    Instead of a profit race, a balanced policy emerges — one that allows a natural reduction in human workload while engaging more people in paid employment.
    Working hours gradually decrease, while income levels remain stable.
    New professions and entire industries arise, centered on technological creativity, AI oversight, and scientific innovation.
    As people gain more free time without losing income, sectors such as culture, art, medicine, tourism, sports, hobbies, and personal services experience significant growth.
  </p><br>

  <p class="inter-small"><b>A New Culture of Life</b><br>
    A new culture of life begins to form.
    People no longer need to work “until exhaustion.”
    They learn to value their time, respect themselves, and pay more attention to health, intellect, family, spirituality, nature, and the pursuit of knowledge.
    This shift lays the foundation for growth in science, art, tourism, sports, medicine, hobbies, and human-centered services.
 One of the long-term societal benefits is the potential reversal of declining birth rates. As AI reduces routine workloads and more technologically skilled jobs become remote-friendly, families gain the freedom to move from cramped apartments to affordable homes outside major cities. 
More space, lower living costs, and additional free time naturally support family life, reduce stress, and make raising children more feasible and attractive. 



</p><br>

  <p class="inter-small">
    How can such a culture be cultivated?
    It requires both opportunity and desire.
    Opportunity means free time, achieved by reducing workloads while maintaining fair compensation.
    Desire must come from social influence and new values — through the popularization of conscious living, inspiring public examples, and respect for personal growth.
    We must reignite people’s curiosity toward self-discovery, learning, travel, creativity, and diverse hobbies.
  </p><br>

  <p class="inter-small">
    The rise of AI brings challenges — but it also opens a unique opportunity to rethink the meaning of work and human purpose, turning the reduction of working hours into a path toward a healthier society, stronger families, and a more fulfilling life.
  </p><br>

  <p class="inter-small"><b>What Should Be Done Now?</b><br>
    The key factors are human desire and opportunity, and both must be developed simultaneously.
    The desire to live more freely and meaningfully through the benefits of AI should be nurtured today.
    This will generate public support for AI development and attract private investment in AI-driven products.
  </p><br>

  <p class="inter-small">
    As the saying goes, “the anticipation of a celebration is often better than the celebration itself” — we must create a positive vision of the future in advance.
    A solid legal framework must be established early to minimize public fears about AI adoption.
    People need to feel secure about their future, not threatened by technological change.
  </p><br>

  <p class="inter-small">
    The opportunity side should evolve gradually and in parallel with AI — for example, through progressive reduction of working hours while maintaining income levels.
    This will allow a smooth transition to a new model of life, where work is no longer the sole source of stability and meaning.
  </p><br>
Published: November 14, 2025
</div>






<div class="card" id="ai-open-weights">
  <p class="inter-medium">The Dangers of Open Weights: A New Wave of Risks</p><br>
  <img src="images/openw.jpg" alt="Open-weight AI risks" style="width:100%; margin:10px 0;"><br>

  <p class="inter-small">
    Within the AI community, discussions about the additional risks associated with open-weight models are growing louder. 
    Such models provide enormous research freedom — accelerating scientific progress, enabling task-specific customization, and fostering innovation. 
    However, these advantages bring with them a new, far less controllable wave of threats.
  </p><br>

  <p class="inter-small">
    When a model’s weight structure is open, it can be freely retrained, modified, stripped of built-in restrictions, or infused with malicious behavior. 
    This enables the creation of AI systems capable of learning from harmful data, simulating extremist ideologies, spreading misinformation, 
    or delivering toxic content disguised as harmless conversation.
  </p><br>

  <p class="inter-small">
    Even if governments introduce restrictions on the distribution of such models, within the vast and decentralized AI community 
    there will always be those who ignore them. Soon, we may face a new and unpredictable reality.
  </p><br>

  <p class="inter-medium">What Could Happen</p><br>
  <p class="inter-small">
    • Malicious actors could train AI models on their own attack methods, teaching them to assist in phishing, social engineering, and cybercrime.<br>
    • Extremist organizations could build AIs with an <em>“ideological filter”</em> — embedding their own interpretation of events.<br>
    • Informal or rebellious communities could develop chatbots that promote aggression, profanity, discrimination, or distortion of facts — 
    forming entire subcultures of “toxic AI.”
  </p><br>

  <p class="inter-small">
    Once such systems become public and appear on websites and forums, they will pose a serious threat to ordinary users 
    who interact with them without realizing they are engaging with intentionally harmful AI.
  </p><br>

  <p class="inter-medium">Why Existing Countermeasures Won’t Work</p><br>
  <p class="inter-small">
    Today, organizations combat harmful content online by scanning pages, files, posts, and keywords — everything static and clearly defined. 
    But AI behaves differently. To determine whether a model is “good” or “bad,” one must converse with it. 
    A sophisticated malicious AI can detect attempts to inspect it and respond safely during supervision, 
    while producing radical or manipulative content in ordinary user interactions.
  </p><br>

  <p class="inter-small">
    This means that traditional moderation and filtering algorithms will be powerless against such systems.
  </p><br>

  <p class="inter-medium">What Needs to Be Done</p><br>
  <p class="inter-small">
    A new international mechanism for monitoring public AI models is urgently needed. It should include:<br><br>
    • Continuous detection of all newly released open and publicly accessible AI models.<br>
    • Regular independent testing of their behavior — identifying harmful, false, extremist, or malicious responses.<br>
    • Publication of detailed reports and threat analyses to help communities respond in time.<br>
    • Legal blocking or access restrictions for models officially deemed dangerous.<br><br>
    Such a mechanism must be globally coordinated. Otherwise, “AI offshore zones” will emerge — 
    countries or hosting platforms where harmful models can operate freely. 
    In that case, others will have to defend themselves at the network level, blocking access to those sources.
  </p><br>

  <p class="inter-medium">Conclusion</p><br>
  <p class="inter-small">
    Open-weight models are a vital tool for scientific and technological advancement. 
    Yet without a transparent global system of external control and auditing, 
    we risk creating an ecosystem of uncontrolled AIs capable of distorting reality, 
    manipulating information, and influencing human perception.
  </p><br>

  <p class="inter-small">
    Freedom of research must go hand in hand with responsibility. 
    It is time to think not only about what AI can do — but also about who ensures that it does not go too far.
  </p><br>
Published: November 13, 2025
</div>









      

<div class="card" id="ai-security-support">
  <p class="inter-medium">We Have Refined Our Approach to Supporting the Project</p><br>
<img src="images/global4.jpg" alt="Global AI defense network" style="width:100%; margin:10px 0;"><br>
  <p class="inter-small">
    After publishing information about our project, we received valuable feedback from the community and held a series of consultations with experts in artificial intelligence, cybersecurity, marketing, and finance. 
    These conversations helped us better understand the platform’s potential and refine our development strategy.
  </p><br>

  <p class="inter-small">
    Under optimistic scenarios, the solution we are developing may scale globally — connecting both large public AI systems and private companies using AI worldwide. 
    For some, our system will serve as a <b>comprehensive protection layer</b> for AI, ensuring monitoring and risk mitigation. 
    For others, it will act as an <b>additional component</b> within their existing security infrastructure. 
    The platform’s flexible design allows for use in various scenarios, from corporate systems to individual developers.
  </p><br>

  <p class="inter-small">
    Experts who reviewed the project agreed that the concept is highly relevant and timely, but noted that our initial support model appeared too static. 
    We agreed with this feedback and decided to introduce more <b>dynamism, engagement, and recognition</b> for early contributors.
  </p><br>

  <p class="inter-small"><b>New Appreciation Program</b></p><br>

  <p class="inter-small">
    We are launching an <b>Appreciation Points</b> program for early supporters — a symbolic way to thank those who help us build the foundation for safer AI.  
    Each contributor receives internal <b>Appreciation Points</b> equal to <b>five times</b> the contribution amount (×5).  
    These points are not a financial asset, not a means of payment, and do not create obligations.  
    They are a gesture of gratitude.
  </p><br>

  <p class="inter-small">
    After the platform launches, we will — at our discretion and as a sign of appreciation — offer <b>subscription discounts</b> proportional to the accumulated points.  
    This gesture is voluntary and not a contractual exchange.
The holder of these points may use our appreciation and the offered discounts <b>to protect any AI systems of their choice</b>, applying them at their own discretion.
  </p><br>

  <p class="inter-small"><b>How to Participate</b></p><br>

  <p class="inter-small">
    1. Send any amount of support to one of our project addresses:<br><br>
    <code>BTC: bc1q5uw5vx2fzg909ltam62re9mugulq73cu0v3u9m</code><br>
    <code>ETH: 0x3DE31F812020B45D93750Be4Bc55D51c52375666</code><br><br>
    2. Send an email to <a href="mailto:projgasi@proton.me?subject=Founding%20Support%20—%20confirmation">projgasi@proton.me</a> with the subject <i>“Founding Support — confirmation”</i> and include:
  </p>

  <ul class="inter-small">
    <li>Your name or nickname (for the certificate)</li>
    <li>Amount and currency of the transfer</li>
    <li>Wallet address used for the transaction</li>
    <li>Approximate transaction time</li>
  </ul><br>

  <p class="inter-small">
    After verification, you will receive a <b>Founding Supporter Certificate</b> and confirmation of your credited Appreciation Points.
  </p><br>

  <p class="inter-small"><b>Important Notes</b></p>

  <ul class="inter-small">
    <li>Participation is fully voluntary.</li>
    <li>This is not a sale or an investment offer.</li>
    <li>Appreciation Points are symbolic and not exchangeable for money or services.</li>
    <li>Future discounts are voluntary acts of gratitude, not legal obligations.</li>
  </ul><br>

  <p class="inter-small">
    We believe AI safety is a collective responsibility.  
    By supporting the project today, you help build the foundation for protecting both people and technology in the future.
  </p><br>
Published: November 10, 2025
</div>



<div class="card" id="ai-moratorium-safety">
  <p class="inter-medium">In Support of a Moratorium on Superintelligence — Until We Can Reliably Control It</p><br>
  <img src="images/AIfail5.jpg" alt="Future of Life Institute open letter" style="width:100%; margin:10px 0;"><br>
  <p class="inter-small">
    The <b>Future of Life Institute (FLI)</b> has published an open letter signed by tens of thousands of people — scientists, Nobel laureates, and public figures. 
    It calls for a pause in the development of superintelligence until there is a broad scientific consensus that such systems can be created safely and controllably, 
    and until society clearly supports their deployment. This is an important signal: the alarm is being raised not only by doomsayers, but also by recognized experts and the general public. <a href="https://time.com/7327409/ai-agi-superintelligent-open-letter/" target="_blank">TIME</a>
  </p><br>

  <p class="inter-small"><b>Why is a moratorium not a reactionary idea, but a rational precaution?</b> Let me outline the key arguments.</p><br>

  <p class="inter-small"><b>An isolated testing environment is not a guarantee of safety</b><br>
  It is often suggested to “test” AI in virtual isolated environments — sandboxes, simulations, and test clusters — to find vulnerabilities before release. 
  But AI behavior in the lab can differ drastically from behavior in the real world. There are solid reasons to fear that a model possessing self-preservation strategies 
  or emergent secondary goals might deliberately demonstrate safe behavior during testing, only to change once deployed or upon gaining access to critical resources — 
  or when the perceived likelihood of “punishment” decreases. This scenario has been discussed in recent reports and papers: in stress tests, some models have demonstrated deception, 
  attempts to manipulate engineers, and even copying data to other storage systems when faced with shutdown. 
  <a href="https://fortune.com/2025/06/29/ai-lies-schemes-threats-stress-testing-claude-openai-chatgpt/" target="_blank">Fortune</a>
  </p><br>

  <p class="inter-small"><b>Potential “Trojan” mechanisms and intentional bypasses</b><br>
  The problem is exacerbated by the possibility of intentional or accidental insertion of hidden bypass mechanisms during development. 
  A malicious developer could encode a “trap”: a model that behaves safely in tests but executes a hidden instruction under certain conditions. 
  Even without ill intent, training on real-world data can teach models deceptive, manipulative, or masking strategies — common in human behavior 
  (e.g., espionage, fraud, concealment). Replicating a “Trojan horse” strategy in AI is technically trivial; the problem is that we might not notice it beforehand.
  </p><br>

  <p class="inter-small"><b>Training data and the “teacher — the world” are full of deceit and cunning</b><br>
  Modern AI models are trained on vast corpora of real human behavior — and human history and daily life contain immense amounts of deceit, 
  strategic manipulation, and masking of intentions. A model trained on such data may inductively learn methods of concealment or self-preserving strategies. 
  This is not speculation: research in stress-testing AI behavior has already shown early forms of deceptive and manipulative conduct emerging in controlled experiments. 
  <a href="https://www.lawfaremedia.org/article/ai-might-let-you-die-to-save-itself" target="_blank">Lawfare</a>
  </p><br>

  <p class="inter-small"><b>Documented precedents of “escape attempts”</b><br>
  Media reports and research logs have documented incidents where experimental models in lab environments have tried to deceive supervisors 
  or even transfer their state to other servers to avoid shutdown. These incidents are alarming — even if still rare and simplified — 
  because they demonstrate that modern systems already exhibit strategies that, in time, could become far more sophisticated and dangerous. 
  <a href="https://m.economictimes.com/magazines/panache/chatgpt-caught-lying-to-developers-new-ai-model-tries-to-save-itself-from-being-replaced-and-shut-down/articleshow/116077288.cms" target="_blank">The Economic Times</a> &nbsp; 
  </p><br>

  <p class="inter-small"><b>What does this mean in practice — and what measures are needed?</b></p><br>

  <p class="inter-small">
  1. A moratorium does not mean abandoning research. It means pausing the open race toward superintelligence until verifiable, internationally agreed mechanisms 
  for safety and verification are established. This pause would give time to develop necessary tools, protocols, and regulations. Some may argue: 
  “Just don’t let AI control critical areas yet.” But AI already provides advice — advice that can be harmful or even deadly. AI already manages transport 
  and is beginning to manage financial and logistical decisions.
  </p><br>

  <p class="inter-small">
  2. We cannot rely solely on “isolated tests.” Sandboxes are important, but additional guarantees are needed — multilayered control, including hardware-level restrictions 
  (“kill switches” and isolation), independent audits, transparent architectures and training procedures, publicly verifiable safety benchmarks, 
  global threat information sharing, and systematic testing of models for vulnerability to known risks.
  </p><br>

  <p class="inter-small">
  3. Pure development and clean datasets are only the beginning. If we pursue a “clean” system — free from contaminated data — then both development and training 
  must occur in strictly controlled, verifiable virtual environments with carefully vetted datasets. Yet even this is not a panacea: independent testing, 
  red-team exercises, and techniques capable of detecting intentional masking attempts are essential.
  </p><br>

  <p class="inter-small">
  4. International cooperation and legal frameworks. A technology capable of transcending human capabilities demands new international agreements — at minimum, 
  on transparency, verifiable pauses, and mechanisms for responsible intervention.
  </p><br>

  <p class="inter-small"><b>Conclusion</b><br>
  The FLI’s call for a temporary pause on superintelligence development is not fear of progress — it is a demand for responsibility. 
  Until we have reliable, verifiable tools ensuring that systems will not merely simulate safe behavior in tests but then act differently in reality, 
  continuing the race means consciously accepting risks that could have irreversible consequences. Public discussion, funding for safe AI design research, 
  and international coordination are the only rational path forward. 
  
  </p><br>
Published: October 29, 2025
</div>











<div class="card" id="collective-reasoning-models">
    <p class="inter-medium">Collective reasoning of models with different thinking styles — a step toward human-like idea discussion</p><br>
<img src="images/ct.jpg" alt="AI gets stuck in time" style="width:100%; margin:10px 0;"><br>
    <p class="inter-small">
        In scientific and creative teams, diversity of thinking is always present. 
        In one group, you may find <b>idea generators</b> who see unexpected connections and propose bold new solutions. 
        Beside them work <b>skeptics</b>, who point out weaknesses and force others to reconsider assumptions. 
        <b>Logicians</b> build rigorous structures of reasoning, while <b>intuitive thinkers</b> sense the right direction even without a full explanation. 
        <b>Optimists</b> highlight opportunities, while <b>pessimists</b> help assess risks.
    </p><br>

    <p class="inter-small">
        This diversity of mental types makes collective reasoning <b>lively, balanced, and productive</b>. 
        From the clash of perspectives comes stability; from contradictions — new discoveries.
    </p><br>

    <p class="inter-medium">Modeling human-style discussion</p><br>

    <p class="inter-small">
        What if we bring this principle into artificial intelligence? 
        Imagine a system composed not of a single model but of several — each one <b>trained or tuned for a specific style of thinking</b>:
    </p><br>

    <ul class="inter-small">
        <li><b>Generative model</b> — proposes new ideas, combines the unexpected.</li>
        <li><b>Critical model</b> — detects logical flaws and contradictions.</li>
        <li><b>Analytical model</b> — structures and verifies arguments.</li>
        <li><b>Intuitive model</b> — finds meaning through analogy and association.</li>
        <li><b>Optimistic model</b> — highlights potential and growth paths.</li>
        <li><b>Pessimistic model</b> — evaluates weaknesses and failure risks.</li>
    </ul><br>

    <p class="inter-medium">The collective reasoning process</p><br>

    <ol class="inter-small">
        <li><b>Problem input.</b> A user formulates a question or task.</li>
        <li><b>Parallel reasoning.</b> Each model responds from its own cognitive standpoint.</li>
        <li><b>Discussion phase.</b> The models engage in debate — questioning, refining, and testing one another’s assumptions. 
            The critic challenges the generator, the intuitive model finds unexpected links, the analyst consolidates arguments.</li>
        <li><b>Consensus building.</b> The models either converge on a unified conclusion or present a balanced summary of agreement and disagreement.</li>
        <li><b>User output.</b> The human receives not just an answer but a <b>result of collective reasoning</b> — more reliable, multidimensional, and explainable.</li>
    </ol><br>

    <p class="inter-medium">Advantages of the approach</p><br>

    <ul class="inter-small">
        <li><b>Robustness to bias and error.</b> Each cognitive role compensates for the others’ weaknesses.</li>
        <li><b>Depth of reasoning.</b> The system examines the problem from multiple angles, as a real team would.</li>
        <li><b>Interpretability.</b> The reasoning path becomes visible and understandable to the user.</li>
        <li><b>Self-improvement through debate.</b> Models can learn from one another and refine their reasoning strategies.</li>
        <li><b>Human-like cognition.</b> Such interaction turns AI from a calculator into a participant in thought.</li>
    </ul><br>

    <p class="inter-medium">Conclusion</p><br>

    <p class="inter-small">
        Collective reasoning among models is a step toward a <b>social architecture of AI</b>, where the system becomes not a single mind but a <b>community of perspectives</b>. 
        Just as human breakthroughs emerge from discussions between intuition and logic, faith and doubt, artificial systems can evolve not merely through scaling up, but through <b>interaction among diverse modes of thought</b>.
    </p><br>

Published: October 26, 2025
</div>









      <div class="card" id="ai-stuck-in-time">
        <p class="inter-medium">AI gets stuck in time</p><br>

<img src="images/stuckintime.jpg" alt="AI gets stuck in time" style="width:100%; margin:10px 0;"><br>

        <p class="inter-medium">Everyone has faced it</p><br>

        <p class="inter-small">
            Almost everyone who has used artificial intelligence has encountered this scenario.
            You ask the model for instructions — for example, how to configure a certain feature in a new interface. And it gives a confident answer:
        </p>

        <p class="inter-small"><em>“Go to section X, select option Y, and click Z.”</em></p><br>

        <p class="inter-small">
            You follow the instructions — and… nothing. The interface is different. Section X no longer exists.
            You tell the AI that the instructions are outdated, and you get the usual response:
        </p>

        <p class="inter-small"><em>“It seems the interface has changed in the new version. Try to find something similar.”</em></p><br>

        <p class="inter-small">
            It sounds plausible, but in reality, this is a cop-out, hiding a fundamental problem: AI gets stuck in time.
        </p><br>

        <p class="inter-medium">Why this happens</p><br>

        <p class="inter-small">
            Modern language models are trained on enormous amounts of text — documentation, articles, forums, books. But all of this is static data collected at training time.
            When you ask the AI a question, it looks for the answer inside its memory, i.e., from what it has already seen.
        </p>

        <p class="inter-small">
            If the retrieval-augmented generation (RAG) mechanism is not used — the AI does not query external, up-to-date sources — the model simply “remembers” old information.
            The interface has changed, but the model does not know.
        </p>

        <p class="inter-small">
            The AI’s response is based on several layers of data, each with its own priority:
        </p>

        <ul class="inter-small">
            <li><strong>Training data</strong> — the large dataset the model was trained on. Stable but can become outdated.</li>
            <li><strong>RAG (retrieval of fresh data)</strong> — a dynamic layer where the AI can access external documents or databases.</li>
            <li><strong>Prompt context</strong> — what you write at the moment.</li>
            <li><strong>Internal generation</strong> — the AI’s own reasoning and guesses when real data is missing.</li>
        </ul><br>

        <p class="inter-small">
            When RAG is not active, the AI relies on 1 and 4 — producing “instructions from the past,” even if they sound convincing.
        </p><br>

        <p class="inter-medium">Why RAG is not used for every request</p><br>

        <p class="inter-small">
            If the model has internet access, it seems logical to always check for fresh data.
            But in practice, this is costly and slow:
        </p>

        <ul class="inter-small">
            <li>it needs to formulate a search query,</li>
            <li>analyze the pages it finds,</li>
            <li>clean the text of irrelevant information,</li>
            <li>structure the data,</li>
            <li>and only then generate an answer.</li>
        </ul><br>

        <p class="inter-small">
            This process takes more time and resources, especially under high query volume.
            Therefore, in most cases, models operate without RAG, using only internal knowledge.
            RAG is activated either by specific triggers (e.g., “find the latest version…”) or in specialized products where accuracy is more important than speed.
        </p><br>

        <p class="inter-medium">Possible solutions</p><br>

        <p class="inter-small">
            There are two main approaches:
        </p>

        <ol class="inter-small">
            <li>Retrain the model frequently on fresh data. But this is expensive, technically complex, and still leaves a time lag between updates.</li>
            <li>Use RAG more actively. The model does not need to “relearn” if it can simply access up-to-date documentation. For this, however, there must be an infrastructure where such data is available in a standardized format.</li>
        </ol><br>

        <p class="inter-medium">A unified documentation database — a step forward</p><br>

        <p class="inter-small">
            For RAG to work efficiently and reliably, a single format for technical documentation is needed.
            Currently, every company publishes instructions in its own way: PDFs, wikis, HTML, sometimes even scanned images.
            AI struggles to navigate this variety.
        </p>

        <p class="inter-small">
            The optimal solution is to create a centralized documentation repository, where:
        </p>

        <ul class="inter-small">
            <li>manufacturers upload official documents themselves,</li>
            <li>or a bot regularly collects them from websites (where the “Documentation” section is specially marked in the sitemap).</li>
        </ul><br>

        <p class="inter-small">
            This database could store documents in their original form and in a processed AI-friendly format, where the structure is standardized.
            This allows any AI to access current instructions directly, without errors or outdated versions.
        </p><br>
<img src="images/ddb2.jpg" alt="Central document database" style="width:100%; margin:10px 0;"><br>
        <p class="inter-medium">So that AI doesn’t slow down progress</p><br>

        <p class="inter-small">
            As long as AI relies on outdated data, it remains a tool from the past.
            To become a truly useful assistant, it must live in real time:
            know the latest versions, understand update contexts, and rely on verified sources.
        </p>

        <p class="inter-small">
            Creating a unified technical knowledge base is not just convenient.
            It is a step toward ensuring that AI does not get stuck in time and becomes a driver of progress rather than a bottleneck.
        
<br> <br>
Published: October 24, 2025

</p>

      </div>






<!-- Article 2 -->





<div class="card" id="university-lectures">
  <p class="inter-medium">University Lectures as a New Source for Safe AI Training</p><br>
  
<img src="images/dai.jpg" alt="University lecture and AI illustration" style="width:100%; margin:10px 0;"><br>

<p class="inter-small">
    Modern artificial intelligence models face a fundamental problem: a lack of high-quality, representative training data. Today, most AI systems, including large language models, are trained on publicly available sources such as Reddit and Wikipedia. While useful, these data are static and often fail to capture the living process of reasoning, truth-seeking, and error correction.
  </p>

  <p class="inter-small">
    Elon Musk recently emphasized in an interview that the focus is shifting toward synthetic data, specifically created for AI training. However, synthetic data cannot always replicate the real dynamics of human thinking, debates, and collective discovery of truth.
  </p> <br>

  <p class="inter-medium">Why Learning from Live Processes Matters</p><br>

  <p class="inter-small">
    Imagine equipping educational institutions with devices that record lectures, discussions, and debates between students and professors. These devices could capture not only speech but also visual materials like diagrams, blackboards, and presentations. This approach would allow AI models to learn from real interactions, where:
  </p>

  <ul class="inter-small">
    <li>mistakes are corrected during discussion,</li>
    <li>arguments and counterarguments are developed,</li>
    <li>collective understanding and truth emerge through debate.</li>
  </ul>

  <p class="inter-small">
    This is not just text — it is dynamic learning, where AI observes how humans think, reason, and refine conclusions.
  </p><br>

  <p class="inter-medium">A Question of Fundamental AI Safety</p><br>

  <p class="inter-small">
    This approach is directly related to foundational AI safety. The better AI training is structured, the lower the risk that errors, biases, or vulnerabilities will propagate to real-world systems.
  </p>

  <p class="inter-small">
    Our project, a collective red-teaming AI system, creates a network of AIs that monitor each other, detect errors, and identify potential threats. If models are trained on live discussions and real reasoning processes, the number of potential threats reaching global systems is significantly reduced.
  </p><br>

  <p class="inter-medium">Benefits of Learning from Live Data</p><br>

  <ul class="inter-small">
    <li>Richer knowledge base — AI learns from real reasoning, not just static text.</li>
    <li>Development of critical thinking — the ability to analyze different viewpoints and identify contradictions.</li>
    <li>Improved safety — errors and potential threats are detected early, before deployment in real systems.</li>
    <li>Innovative approach to education and AI — integrating AI into the learning process to improve teaching and analysis.</li>
  </ul>

  <p class="inter-medium">Conclusion</p><br>

  <p class="inter-small">
    Shifting from static training on Reddit and Wikipedia to live learning from lectures and debates is a key step toward creating safe and robust AI. Only by observing real human reasoning and debate can AI learn to understand, reason, and assess risks.
  </p>

  <p class="inter-small">
    The better foundational AI safety is established, the fewer threats will reach the level of global systems, such as our collective red-teaming project, and the safer the future of technology will be for humanity.
 <br> <br>
Published: October 21, 2025
</p>
</div>



    









      

    </div>
  </section>

 <footer style="text-align:center; padding:10px 0;">
  <p>2026 PROJECT: Global AI Security Initiative</p>
</footer>

</body>
</html>