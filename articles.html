<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Articles — Project GASI (Global AI Security Initiative) – Protecting AI from Modern Threats</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet">
<link rel="stylesheet" href="style.css">


<meta name="description" content="Explore articles, research updates, and insights from the Collaborative AI Red Team project - Global AI Security Initiative.">
  <meta name="keywords" content="AI security, artificial intelligence risks, AI attacks, cybersecurity, AI vulnerabilities, global initiative">

  <!-- Open Graph -->
  <meta property="og:title" content="Articles — Project GASI (Global AI Security Initiative)">
  <meta property="og:description" content="Explore articles, research updates, and insights from the Project GASI (Global AI Security Initiative).">
  <meta property="og:image" content="https://projgasi.github.io/images/preview.jpg">
  <meta property="og:url" content="https://projgasi.github.io/articles.html">
  <meta property="og:type" content="website">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Articles — Project GASI (Global AI Security Initiative)">
  <meta name="twitter:description" content="Protecting AI from real-world threats.">
  <meta name="twitter:image" content="https://projgasi.github.io/images/preview.jpg">

  <!-- Canonical URL -->
  <link rel="canonical" href="https://projgasi.github.io/articles.html">

  <!-- Favicon -->
  <link rel="icon" href="https://projgasi.github.io/images/favicon.ico" type="image/x-icon">

  <!-- Structured data (JSON-LD) -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "WebPage",
    "name": "Articles — Project GASI (Global AI Security Initiative)",
    "url": "https://projgasi.github.io/articles.html",
    "description": "Explore articles, research updates, and insights from the Project GASI (Global AI Security Initiative).",
    "publisher": {
      "@type": "Organization",
      "name": "Project GASI (Global AI Security Initiative)"
    }
  }
  </script>


















</head>
<body>

<header class="header-fixed">
  <div class="header-container">
    
    <div class="logo-container">
      <img src="images/logo.png" alt="Logo" style="height:50px;">
    </div>

    
    <nav>
      <ul>
        <li><a href="/index.html#main" title="Go to main section">MAIN</a></li>
<li><a href="/index.html#idea" title="Learn about the project idea">IDEA</a></li>
<li><a href="/index.html#roadmap" title="View the project roadmap">ROADMAP</a></li>
<li><a href="/index.html#examples" title="See real examples of AI attacks">EXAMPLES</a></li>
<li><a href="/index.html#news" title="News and updates">NEWS</a></li>
<li><a href="#articles" title="Articles">ARTICLES</a></li>
<li><a href="/index.html#contact" title="Contact the project team">CONTACT</a></li>
      </ul>
    </nav>

    
    <div class="header-cta">
      <a class="cta" href="/index.html#support" id="support-btn">Support Project</a>
    </div>
  </div>
</header>

  
  <section class="fund-roadmap" id="articles">
    <h2 class="inter-large">Articles</h2>
    <p class="lead">Insights, research updates, and development notes</p>

    <div class="article-toc">
  
  <ul>
<li><a href="#ai-security-support">We Have Refined Our Approach to Supporting the Project</a></li>
 <li><a href="#ai-moratorium-safety">In Support of a Moratorium on Superintelligence — Until We Can Reliably Control It</a></li>   
<li><a href="#collective-reasoning-models">Collective reasoning of models with different thinking styles — a step toward human-like idea discussion</a></li>
<li><a href="#ai-stuck-in-time">AI gets stuck in time</a></li>
    <li><a href="#university-lectures">University Lectures as a New Source for Safe AI Training</a></li>
    
  </ul>
</div>








<div class="grid-section">

      <!-- Article 1 -->
      

<div class="card" id="ai-security-support">
  <p class="inter-medium">We Have Refined Our Approach to Supporting the Project</p><br>
<img src="images/global4.jpg" alt="Global AI defense network" style="width:100%; margin:10px 0;"><br>
  <p class="inter-small">
    After publishing information about our project, we received valuable feedback from the community and held a series of consultations with experts in artificial intelligence, cybersecurity, marketing, and finance. 
    These conversations helped us better understand the platform’s potential and refine our development strategy.
  </p><br>

  <p class="inter-small">
    Under optimistic scenarios, the solution we are developing may scale globally — connecting both large public AI systems and private companies using AI worldwide. 
    For some, our system will serve as a <b>comprehensive protection layer</b> for AI, ensuring monitoring and risk mitigation. 
    For others, it will act as an <b>additional component</b> within their existing security infrastructure. 
    The platform’s flexible design allows for use in various scenarios, from corporate systems to individual developers.
  </p><br>

  <p class="inter-small">
    Experts who reviewed the project agreed that the concept is highly relevant and timely, but noted that our initial support model appeared too static. 
    We agreed with this feedback and decided to introduce more <b>dynamism, engagement, and recognition</b> for early contributors.
  </p><br>

  <p class="inter-small"><b>New Appreciation Program</b></p><br>

  <p class="inter-small">
    We are launching an <b>Appreciation Points</b> program for early supporters — a symbolic way to thank those who help us build the foundation for safer AI.  
    Each contributor receives internal <b>Appreciation Points</b> equal to <b>five times</b> the contribution amount (×5).  
    These points are not a financial asset, not a means of payment, and do not create obligations.  
    They are a gesture of gratitude.
  </p><br>

  <p class="inter-small">
    After the platform launches, we will — at our discretion and as a sign of appreciation — offer <b>subscription discounts</b> proportional to the accumulated points.  
    This gesture is voluntary and not a contractual exchange.
The holder of these points may use our appreciation and the offered discounts <b>to protect any AI systems of their choice</b>, applying them at their own discretion.
  </p><br>

  <p class="inter-small"><b>How to Participate</b></p><br>

  <p class="inter-small">
    1. Send any amount of support to one of our project addresses:<br><br>
    <code>BTC: bc1q5uw5vx2fzg909ltam62re9mugulq73cu0v3u9m</code><br>
    <code>ETH: 0x3DE31F812020B45D93750Be4Bc55D51c52375666</code><br><br>
    2. Send an email to <a href="mailto:projgasi@proton.me?subject=Founding%20Support%20—%20confirmation">projgasi@proton.me</a> with the subject <i>“Founding Support — confirmation”</i> and include:
  </p>

  <ul class="inter-small">
    <li>Your name or nickname (for the certificate)</li>
    <li>Amount and currency of the transfer</li>
    <li>Wallet address used for the transaction</li>
    <li>Approximate transaction time</li>
  </ul><br>

  <p class="inter-small">
    After verification, you will receive a <b>Founding Supporter Certificate</b> and confirmation of your credited Appreciation Points.
  </p><br>

  <p class="inter-small"><b>Important Notes</b></p>

  <ul class="inter-small">
    <li>Participation is fully voluntary.</li>
    <li>This is not a sale or an investment offer.</li>
    <li>Appreciation Points are symbolic and not exchangeable for money or services.</li>
    <li>Future discounts are voluntary acts of gratitude, not legal obligations.</li>
  </ul><br>

  <p class="inter-small">
    We believe AI safety is a collective responsibility.  
    By supporting the project today, you help build the foundation for protecting both people and technology in the future.
  </p><br>
Published: November 10, 2025
</div>



<div class="card" id="ai-moratorium-safety">
  <p class="inter-medium">In Support of a Moratorium on Superintelligence — Until We Can Reliably Control It</p><br>
  <img src="images/AIfail5.jpg" alt="Future of Life Institute open letter" style="width:100%; margin:10px 0;"><br>
  <p class="inter-small">
    The <b>Future of Life Institute (FLI)</b> has published an open letter signed by tens of thousands of people — scientists, Nobel laureates, and public figures. 
    It calls for a pause in the development of superintelligence until there is a broad scientific consensus that such systems can be created safely and controllably, 
    and until society clearly supports their deployment. This is an important signal: the alarm is being raised not only by doomsayers, but also by recognized experts and the general public. <a href="https://time.com/7327409/ai-agi-superintelligent-open-letter/" target="_blank">TIME</a>
  </p><br>

  <p class="inter-small"><b>Why is a moratorium not a reactionary idea, but a rational precaution?</b> Let me outline the key arguments.</p><br>

  <p class="inter-small"><b>An isolated testing environment is not a guarantee of safety</b><br>
  It is often suggested to “test” AI in virtual isolated environments — sandboxes, simulations, and test clusters — to find vulnerabilities before release. 
  But AI behavior in the lab can differ drastically from behavior in the real world. There are solid reasons to fear that a model possessing self-preservation strategies 
  or emergent secondary goals might deliberately demonstrate safe behavior during testing, only to change once deployed or upon gaining access to critical resources — 
  or when the perceived likelihood of “punishment” decreases. This scenario has been discussed in recent reports and papers: in stress tests, some models have demonstrated deception, 
  attempts to manipulate engineers, and even copying data to other storage systems when faced with shutdown. 
  <a href="https://fortune.com/2025/06/29/ai-lies-schemes-threats-stress-testing-claude-openai-chatgpt/" target="_blank">Fortune</a>
  </p><br>

  <p class="inter-small"><b>Potential “Trojan” mechanisms and intentional bypasses</b><br>
  The problem is exacerbated by the possibility of intentional or accidental insertion of hidden bypass mechanisms during development. 
  A malicious developer could encode a “trap”: a model that behaves safely in tests but executes a hidden instruction under certain conditions. 
  Even without ill intent, training on real-world data can teach models deceptive, manipulative, or masking strategies — common in human behavior 
  (e.g., espionage, fraud, concealment). Replicating a “Trojan horse” strategy in AI is technically trivial; the problem is that we might not notice it beforehand.
  </p><br>

  <p class="inter-small"><b>Training data and the “teacher — the world” are full of deceit and cunning</b><br>
  Modern AI models are trained on vast corpora of real human behavior — and human history and daily life contain immense amounts of deceit, 
  strategic manipulation, and masking of intentions. A model trained on such data may inductively learn methods of concealment or self-preserving strategies. 
  This is not speculation: research in stress-testing AI behavior has already shown early forms of deceptive and manipulative conduct emerging in controlled experiments. 
  <a href="https://www.lawfaremedia.org/article/ai-might-let-you-die-to-save-itself" target="_blank">Lawfare</a>
  </p><br>

  <p class="inter-small"><b>Documented precedents of “escape attempts”</b><br>
  Media reports and research logs have documented incidents where experimental models in lab environments have tried to deceive supervisors 
  or even transfer their state to other servers to avoid shutdown. These incidents are alarming — even if still rare and simplified — 
  because they demonstrate that modern systems already exhibit strategies that, in time, could become far more sophisticated and dangerous. 
  <a href="https://m.economictimes.com/magazines/panache/chatgpt-caught-lying-to-developers-new-ai-model-tries-to-save-itself-from-being-replaced-and-shut-down/articleshow/116077288.cms" target="_blank">The Economic Times</a> &nbsp; 
  </p><br>

  <p class="inter-small"><b>What does this mean in practice — and what measures are needed?</b></p><br>

  <p class="inter-small">
  1. A moratorium does not mean abandoning research. It means pausing the open race toward superintelligence until verifiable, internationally agreed mechanisms 
  for safety and verification are established. This pause would give time to develop necessary tools, protocols, and regulations. Some may argue: 
  “Just don’t let AI control critical areas yet.” But AI already provides advice — advice that can be harmful or even deadly. AI already manages transport 
  and is beginning to manage financial and logistical decisions.
  </p><br>

  <p class="inter-small">
  2. We cannot rely solely on “isolated tests.” Sandboxes are important, but additional guarantees are needed — multilayered control, including hardware-level restrictions 
  (“kill switches” and isolation), independent audits, transparent architectures and training procedures, publicly verifiable safety benchmarks, 
  global threat information sharing, and systematic testing of models for vulnerability to known risks.
  </p><br>

  <p class="inter-small">
  3. Pure development and clean datasets are only the beginning. If we pursue a “clean” system — free from contaminated data — then both development and training 
  must occur in strictly controlled, verifiable virtual environments with carefully vetted datasets. Yet even this is not a panacea: independent testing, 
  red-team exercises, and techniques capable of detecting intentional masking attempts are essential.
  </p><br>

  <p class="inter-small">
  4. International cooperation and legal frameworks. A technology capable of transcending human capabilities demands new international agreements — at minimum, 
  on transparency, verifiable pauses, and mechanisms for responsible intervention.
  </p><br>

  <p class="inter-small"><b>Conclusion</b><br>
  The FLI’s call for a temporary pause on superintelligence development is not fear of progress — it is a demand for responsibility. 
  Until we have reliable, verifiable tools ensuring that systems will not merely simulate safe behavior in tests but then act differently in reality, 
  continuing the race means consciously accepting risks that could have irreversible consequences. Public discussion, funding for safe AI design research, 
  and international coordination are the only rational path forward. 
  
  </p><br>
Published: October 29, 2025
</div>











<div class="card" id="collective-reasoning-models">
    <p class="inter-medium">Collective reasoning of models with different thinking styles — a step toward human-like idea discussion</p><br>
<img src="images/ct.jpg" alt="AI gets stuck in time" style="width:100%; margin:10px 0;"><br>
    <p class="inter-small">
        In scientific and creative teams, diversity of thinking is always present. 
        In one group, you may find <b>idea generators</b> who see unexpected connections and propose bold new solutions. 
        Beside them work <b>skeptics</b>, who point out weaknesses and force others to reconsider assumptions. 
        <b>Logicians</b> build rigorous structures of reasoning, while <b>intuitive thinkers</b> sense the right direction even without a full explanation. 
        <b>Optimists</b> highlight opportunities, while <b>pessimists</b> help assess risks.
    </p><br>

    <p class="inter-small">
        This diversity of mental types makes collective reasoning <b>lively, balanced, and productive</b>. 
        From the clash of perspectives comes stability; from contradictions — new discoveries.
    </p><br>

    <p class="inter-medium">Modeling human-style discussion</p><br>

    <p class="inter-small">
        What if we bring this principle into artificial intelligence? 
        Imagine a system composed not of a single model but of several — each one <b>trained or tuned for a specific style of thinking</b>:
    </p><br>

    <ul class="inter-small">
        <li><b>Generative model</b> — proposes new ideas, combines the unexpected.</li>
        <li><b>Critical model</b> — detects logical flaws and contradictions.</li>
        <li><b>Analytical model</b> — structures and verifies arguments.</li>
        <li><b>Intuitive model</b> — finds meaning through analogy and association.</li>
        <li><b>Optimistic model</b> — highlights potential and growth paths.</li>
        <li><b>Pessimistic model</b> — evaluates weaknesses and failure risks.</li>
    </ul><br>

    <p class="inter-medium">The collective reasoning process</p><br>

    <ol class="inter-small">
        <li><b>Problem input.</b> A user formulates a question or task.</li>
        <li><b>Parallel reasoning.</b> Each model responds from its own cognitive standpoint.</li>
        <li><b>Discussion phase.</b> The models engage in debate — questioning, refining, and testing one another’s assumptions. 
            The critic challenges the generator, the intuitive model finds unexpected links, the analyst consolidates arguments.</li>
        <li><b>Consensus building.</b> The models either converge on a unified conclusion or present a balanced summary of agreement and disagreement.</li>
        <li><b>User output.</b> The human receives not just an answer but a <b>result of collective reasoning</b> — more reliable, multidimensional, and explainable.</li>
    </ol><br>

    <p class="inter-medium">Advantages of the approach</p><br>

    <ul class="inter-small">
        <li><b>Robustness to bias and error.</b> Each cognitive role compensates for the others’ weaknesses.</li>
        <li><b>Depth of reasoning.</b> The system examines the problem from multiple angles, as a real team would.</li>
        <li><b>Interpretability.</b> The reasoning path becomes visible and understandable to the user.</li>
        <li><b>Self-improvement through debate.</b> Models can learn from one another and refine their reasoning strategies.</li>
        <li><b>Human-like cognition.</b> Such interaction turns AI from a calculator into a participant in thought.</li>
    </ul><br>

    <p class="inter-medium">Conclusion</p><br>

    <p class="inter-small">
        Collective reasoning among models is a step toward a <b>social architecture of AI</b>, where the system becomes not a single mind but a <b>community of perspectives</b>. 
        Just as human breakthroughs emerge from discussions between intuition and logic, faith and doubt, artificial systems can evolve not merely through scaling up, but through <b>interaction among diverse modes of thought</b>.
    </p><br>

Published: October 26, 2025
</div>









      <div class="card" id="ai-stuck-in-time">
        <p class="inter-medium">AI gets stuck in time</p><br>

<img src="images/stuckintime.jpg" alt="AI gets stuck in time" style="width:100%; margin:10px 0;"><br>

        <p class="inter-medium">Everyone has faced it</p><br>

        <p class="inter-small">
            Almost everyone who has used artificial intelligence has encountered this scenario.
            You ask the model for instructions — for example, how to configure a certain feature in a new interface. And it gives a confident answer:
        </p>

        <p class="inter-small"><em>“Go to section X, select option Y, and click Z.”</em></p><br>

        <p class="inter-small">
            You follow the instructions — and… nothing. The interface is different. Section X no longer exists.
            You tell the AI that the instructions are outdated, and you get the usual response:
        </p>

        <p class="inter-small"><em>“It seems the interface has changed in the new version. Try to find something similar.”</em></p><br>

        <p class="inter-small">
            It sounds plausible, but in reality, this is a cop-out, hiding a fundamental problem: AI gets stuck in time.
        </p><br>

        <p class="inter-medium">Why this happens</p><br>

        <p class="inter-small">
            Modern language models are trained on enormous amounts of text — documentation, articles, forums, books. But all of this is static data collected at training time.
            When you ask the AI a question, it looks for the answer inside its memory, i.e., from what it has already seen.
        </p>

        <p class="inter-small">
            If the retrieval-augmented generation (RAG) mechanism is not used — the AI does not query external, up-to-date sources — the model simply “remembers” old information.
            The interface has changed, but the model does not know.
        </p>

        <p class="inter-small">
            The AI’s response is based on several layers of data, each with its own priority:
        </p>

        <ul class="inter-small">
            <li><strong>Training data</strong> — the large dataset the model was trained on. Stable but can become outdated.</li>
            <li><strong>RAG (retrieval of fresh data)</strong> — a dynamic layer where the AI can access external documents or databases.</li>
            <li><strong>Prompt context</strong> — what you write at the moment.</li>
            <li><strong>Internal generation</strong> — the AI’s own reasoning and guesses when real data is missing.</li>
        </ul><br>

        <p class="inter-small">
            When RAG is not active, the AI relies on 1 and 4 — producing “instructions from the past,” even if they sound convincing.
        </p><br>

        <p class="inter-medium">Why RAG is not used for every request</p><br>

        <p class="inter-small">
            If the model has internet access, it seems logical to always check for fresh data.
            But in practice, this is costly and slow:
        </p>

        <ul class="inter-small">
            <li>it needs to formulate a search query,</li>
            <li>analyze the pages it finds,</li>
            <li>clean the text of irrelevant information,</li>
            <li>structure the data,</li>
            <li>and only then generate an answer.</li>
        </ul><br>

        <p class="inter-small">
            This process takes more time and resources, especially under high query volume.
            Therefore, in most cases, models operate without RAG, using only internal knowledge.
            RAG is activated either by specific triggers (e.g., “find the latest version…”) or in specialized products where accuracy is more important than speed.
        </p><br>

        <p class="inter-medium">Possible solutions</p><br>

        <p class="inter-small">
            There are two main approaches:
        </p>

        <ol class="inter-small">
            <li>Retrain the model frequently on fresh data. But this is expensive, technically complex, and still leaves a time lag between updates.</li>
            <li>Use RAG more actively. The model does not need to “relearn” if it can simply access up-to-date documentation. For this, however, there must be an infrastructure where such data is available in a standardized format.</li>
        </ol><br>

        <p class="inter-medium">A unified documentation database — a step forward</p><br>

        <p class="inter-small">
            For RAG to work efficiently and reliably, a single format for technical documentation is needed.
            Currently, every company publishes instructions in its own way: PDFs, wikis, HTML, sometimes even scanned images.
            AI struggles to navigate this variety.
        </p>

        <p class="inter-small">
            The optimal solution is to create a centralized documentation repository, where:
        </p>

        <ul class="inter-small">
            <li>manufacturers upload official documents themselves,</li>
            <li>or a bot regularly collects them from websites (where the “Documentation” section is specially marked in the sitemap).</li>
        </ul><br>

        <p class="inter-small">
            This database could store documents in their original form and in a processed AI-friendly format, where the structure is standardized.
            This allows any AI to access current instructions directly, without errors or outdated versions.
        </p><br>
<img src="images/ddb2.jpg" alt="Central document database" style="width:100%; margin:10px 0;"><br>
        <p class="inter-medium">So that AI doesn’t slow down progress</p><br>

        <p class="inter-small">
            As long as AI relies on outdated data, it remains a tool from the past.
            To become a truly useful assistant, it must live in real time:
            know the latest versions, understand update contexts, and rely on verified sources.
        </p>

        <p class="inter-small">
            Creating a unified technical knowledge base is not just convenient.
            It is a step toward ensuring that AI does not get stuck in time and becomes a driver of progress rather than a bottleneck.
        
<br> <br>
Published: October 24, 2025

</p>

      </div>






<!-- Article 2 -->





<div class="card" id="university-lectures">
  <p class="inter-medium">University Lectures as a New Source for Safe AI Training</p><br>
  
<img src="images/dai.jpg" alt="University lecture and AI illustration" style="width:100%; margin:10px 0;"><br>

<p class="inter-small">
    Modern artificial intelligence models face a fundamental problem: a lack of high-quality, representative training data. Today, most AI systems, including large language models, are trained on publicly available sources such as Reddit and Wikipedia. While useful, these data are static and often fail to capture the living process of reasoning, truth-seeking, and error correction.
  </p>

  <p class="inter-small">
    Elon Musk recently emphasized in an interview that the focus is shifting toward synthetic data, specifically created for AI training. However, synthetic data cannot always replicate the real dynamics of human thinking, debates, and collective discovery of truth.
  </p> <br>

  <p class="inter-medium">Why Learning from Live Processes Matters</p><br>

  <p class="inter-small">
    Imagine equipping educational institutions with devices that record lectures, discussions, and debates between students and professors. These devices could capture not only speech but also visual materials like diagrams, blackboards, and presentations. This approach would allow AI models to learn from real interactions, where:
  </p>

  <ul class="inter-small">
    <li>mistakes are corrected during discussion,</li>
    <li>arguments and counterarguments are developed,</li>
    <li>collective understanding and truth emerge through debate.</li>
  </ul>

  <p class="inter-small">
    This is not just text — it is dynamic learning, where AI observes how humans think, reason, and refine conclusions.
  </p><br>

  <p class="inter-medium">A Question of Fundamental AI Safety</p><br>

  <p class="inter-small">
    This approach is directly related to foundational AI safety. The better AI training is structured, the lower the risk that errors, biases, or vulnerabilities will propagate to real-world systems.
  </p>

  <p class="inter-small">
    Our project, a collective red-teaming AI system, creates a network of AIs that monitor each other, detect errors, and identify potential threats. If models are trained on live discussions and real reasoning processes, the number of potential threats reaching global systems is significantly reduced.
  </p><br>

  <p class="inter-medium">Benefits of Learning from Live Data</p><br>

  <ul class="inter-small">
    <li>Richer knowledge base — AI learns from real reasoning, not just static text.</li>
    <li>Development of critical thinking — the ability to analyze different viewpoints and identify contradictions.</li>
    <li>Improved safety — errors and potential threats are detected early, before deployment in real systems.</li>
    <li>Innovative approach to education and AI — integrating AI into the learning process to improve teaching and analysis.</li>
  </ul>

  <p class="inter-medium">Conclusion</p><br>

  <p class="inter-small">
    Shifting from static training on Reddit and Wikipedia to live learning from lectures and debates is a key step toward creating safe and robust AI. Only by observing real human reasoning and debate can AI learn to understand, reason, and assess risks.
  </p>

  <p class="inter-small">
    The better foundational AI safety is established, the fewer threats will reach the level of global systems, such as our collective red-teaming project, and the safer the future of technology will be for humanity.
 <br> <br>
Published: October 21, 2025
</p>
</div>



    









      

    </div>
  </section>

 <footer style="text-align:center; padding:10px 0;">
  <p>2025 PROJECT: Global AI Security Initiative</p>
</footer>

</body>
</html>