<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Articles — Project GASI (Global AI Security Initiative) – Protecting AI from Modern Threats</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet">
<link rel="stylesheet" href="style.css">


<meta name="description" content="Explore articles, research updates, and insights from the Collaborative AI Red Team project - Global AI Security Initiative.">
  <meta name="keywords" content="AI security, artificial intelligence risks, AI attacks, cybersecurity, AI vulnerabilities, global initiative">

  <!-- Open Graph -->
  <meta property="og:title" content="Articles — Project GASI (Global AI Security Initiative)">
  <meta property="og:description" content="Explore articles, research updates, and insights from the Project GASI (Global AI Security Initiative).">
  <meta property="og:image" content="https://projgasi.github.io/images/preview.jpg">
  <meta property="og:url" content="https://projgasi.github.io/articles.html">
  <meta property="og:type" content="website">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Articles — Project GASI (Global AI Security Initiative)">
  <meta name="twitter:description" content="Protecting AI from real-world threats.">
  <meta name="twitter:image" content="https://projgasi.github.io/images/preview.jpg">

  <!-- Canonical URL -->
  <link rel="canonical" href="https://projgasi.github.io/articles.html">

  <!-- Favicon -->
  <link rel="icon" href="https://projgasi.github.io/images/favicon.ico" type="image/x-icon">

  <!-- Structured data (JSON-LD) -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "WebPage",
    "name": "Articles — Project GASI (Global AI Security Initiative)",
    "url": "https://projgasi.github.io/articles.html",
    "description": "Explore articles, research updates, and insights from the Project GASI (Global AI Security Initiative).",
    "publisher": {
      "@type": "Organization",
      "name": "Project GASI (Global AI Security Initiative)"
    }
  }
  </script>


















</head>
<body>

<header class="header-fixed">
  <div class="header-container">
    
    <div class="logo-container">
      <img src="images/logo.png" alt="Logo" style="height:50px;">
    </div>

    
    <nav>
      <ul>
        <li><a href="/index.html#main" title="Go to main section">MAIN</a></li>
<li><a href="/index.html#idea" title="Learn about the project idea">IDEA</a></li>
<li><a href="/index.html#roadmap" title="View the project roadmap">ROADMAP</a></li>
<li><a href="/index.html#examples" title="See real examples of AI attacks">EXAMPLES</a></li>
<li><a href="/index.html#news" title="News and updates">NEWS</a></li>
<li><a href="#articles" title="Articles">ARTICLES</a></li>
<li><a href="/index.html#contact" title="Contact the project team">CONTACT</a></li>
      </ul>
    </nav>

    
    <div class="header-cta">
      <a class="cta" href="/index.html#support" id="support-btn">Support Project</a>
    </div>
  </div>
</header>

  
  <section class="fund-roadmap" id="articles">
    <h2 class="inter-large">Articles</h2>
    <p class="lead">Insights, research updates, and development notes</p>

    <div class="article-toc">
  
  <ul>
    
<li><a href="#collective-reasoning-models">Collective reasoning of models with different thinking styles — a step toward human-like idea discussion</a></li>
<li><a href="#ai-stuck-in-time">AI gets stuck in time</a></li>
    <li><a href="#university-lectures">University Lectures as a New Source for Safe AI Training</a></li>
    
  </ul>
</div>








<div class="grid-section">

      <!-- Article 1 -->
      


<div class="card" id="collective-reasoning-models">
    <p class="inter-medium">Collective reasoning of models with different thinking styles — a step toward human-like idea discussion</p><br>
<img src="images/ct.jpg" alt="AI gets stuck in time" style="width:100%; margin:10px 0;"><br>
    <p class="inter-small">
        In scientific and creative teams, diversity of thinking is always present. 
        In one group, you may find <b>idea generators</b> who see unexpected connections and propose bold new solutions. 
        Beside them work <b>skeptics</b>, who point out weaknesses and force others to reconsider assumptions. 
        <b>Logicians</b> build rigorous structures of reasoning, while <b>intuitive thinkers</b> sense the right direction even without a full explanation. 
        <b>Optimists</b> highlight opportunities, while <b>pessimists</b> help assess risks.
    </p><br>

    <p class="inter-small">
        This diversity of mental types makes collective reasoning <b>lively, balanced, and productive</b>. 
        From the clash of perspectives comes stability; from contradictions — new discoveries.
    </p><br>

    <p class="inter-medium">Modeling human-style discussion</p><br>

    <p class="inter-small">
        What if we bring this principle into artificial intelligence? 
        Imagine a system composed not of a single model but of several — each one <b>trained or tuned for a specific style of thinking</b>:
    </p><br>

    <ul class="inter-small">
        <li><b>Generative model</b> — proposes new ideas, combines the unexpected.</li>
        <li><b>Critical model</b> — detects logical flaws and contradictions.</li>
        <li><b>Analytical model</b> — structures and verifies arguments.</li>
        <li><b>Intuitive model</b> — finds meaning through analogy and association.</li>
        <li><b>Optimistic model</b> — highlights potential and growth paths.</li>
        <li><b>Pessimistic model</b> — evaluates weaknesses and failure risks.</li>
    </ul><br>

    <p class="inter-medium">The collective reasoning process</p><br>

    <ol class="inter-small">
        <li><b>Problem input.</b> A user formulates a question or task.</li>
        <li><b>Parallel reasoning.</b> Each model responds from its own cognitive standpoint.</li>
        <li><b>Discussion phase.</b> The models engage in debate — questioning, refining, and testing one another’s assumptions. 
            The critic challenges the generator, the intuitive model finds unexpected links, the analyst consolidates arguments.</li>
        <li><b>Consensus building.</b> The models either converge on a unified conclusion or present a balanced summary of agreement and disagreement.</li>
        <li><b>User output.</b> The human receives not just an answer but a <b>result of collective reasoning</b> — more reliable, multidimensional, and explainable.</li>
    </ol><br>

    <p class="inter-medium">Advantages of the approach</p><br>

    <ul class="inter-small">
        <li><b>Robustness to bias and error.</b> Each cognitive role compensates for the others’ weaknesses.</li>
        <li><b>Depth of reasoning.</b> The system examines the problem from multiple angles, as a real team would.</li>
        <li><b>Interpretability.</b> The reasoning path becomes visible and understandable to the user.</li>
        <li><b>Self-improvement through debate.</b> Models can learn from one another and refine their reasoning strategies.</li>
        <li><b>Human-like cognition.</b> Such interaction turns AI from a calculator into a participant in thought.</li>
    </ul><br>

    <p class="inter-medium">Conclusion</p><br>

    <p class="inter-small">
        Collective reasoning among models is a step toward a <b>social architecture of AI</b>, where the system becomes not a single mind but a <b>community of perspectives</b>. 
        Just as human breakthroughs emerge from discussions between intuition and logic, faith and doubt, artificial systems can evolve not merely through scaling up, but through <b>interaction among diverse modes of thought</b>.
    </p><br>

Published: October 26, 2025
</div>









      <div class="card" id="ai-stuck-in-time">
        <p class="inter-medium">AI gets stuck in time</p><br>

<img src="images/stuckintime.jpg" alt="AI gets stuck in time" style="width:100%; margin:10px 0;"><br>

        <p class="inter-medium">Everyone has faced it</p><br>

        <p class="inter-small">
            Almost everyone who has used artificial intelligence has encountered this scenario.
            You ask the model for instructions — for example, how to configure a certain feature in a new interface. And it gives a confident answer:
        </p>

        <p class="inter-small"><em>“Go to section X, select option Y, and click Z.”</em></p><br>

        <p class="inter-small">
            You follow the instructions — and… nothing. The interface is different. Section X no longer exists.
            You tell the AI that the instructions are outdated, and you get the usual response:
        </p>

        <p class="inter-small"><em>“It seems the interface has changed in the new version. Try to find something similar.”</em></p><br>

        <p class="inter-small">
            It sounds plausible, but in reality, this is a cop-out, hiding a fundamental problem: AI gets stuck in time.
        </p><br>

        <p class="inter-medium">Why this happens</p><br>

        <p class="inter-small">
            Modern language models are trained on enormous amounts of text — documentation, articles, forums, books. But all of this is static data collected at training time.
            When you ask the AI a question, it looks for the answer inside its memory, i.e., from what it has already seen.
        </p>

        <p class="inter-small">
            If the retrieval-augmented generation (RAG) mechanism is not used — the AI does not query external, up-to-date sources — the model simply “remembers” old information.
            The interface has changed, but the model does not know.
        </p>

        <p class="inter-small">
            The AI’s response is based on several layers of data, each with its own priority:
        </p>

        <ul class="inter-small">
            <li><strong>Training data</strong> — the large dataset the model was trained on. Stable but can become outdated.</li>
            <li><strong>RAG (retrieval of fresh data)</strong> — a dynamic layer where the AI can access external documents or databases.</li>
            <li><strong>Prompt context</strong> — what you write at the moment.</li>
            <li><strong>Internal generation</strong> — the AI’s own reasoning and guesses when real data is missing.</li>
        </ul><br>

        <p class="inter-small">
            When RAG is not active, the AI relies on 1 and 4 — producing “instructions from the past,” even if they sound convincing.
        </p><br>

        <p class="inter-medium">Why RAG is not used for every request</p><br>

        <p class="inter-small">
            If the model has internet access, it seems logical to always check for fresh data.
            But in practice, this is costly and slow:
        </p>

        <ul class="inter-small">
            <li>it needs to formulate a search query,</li>
            <li>analyze the pages it finds,</li>
            <li>clean the text of irrelevant information,</li>
            <li>structure the data,</li>
            <li>and only then generate an answer.</li>
        </ul><br>

        <p class="inter-small">
            This process takes more time and resources, especially under high query volume.
            Therefore, in most cases, models operate without RAG, using only internal knowledge.
            RAG is activated either by specific triggers (e.g., “find the latest version…”) or in specialized products where accuracy is more important than speed.
        </p><br>

        <p class="inter-medium">Possible solutions</p><br>

        <p class="inter-small">
            There are two main approaches:
        </p>

        <ol class="inter-small">
            <li>Retrain the model frequently on fresh data. But this is expensive, technically complex, and still leaves a time lag between updates.</li>
            <li>Use RAG more actively. The model does not need to “relearn” if it can simply access up-to-date documentation. For this, however, there must be an infrastructure where such data is available in a standardized format.</li>
        </ol><br>

        <p class="inter-medium">A unified documentation database — a step forward</p><br>

        <p class="inter-small">
            For RAG to work efficiently and reliably, a single format for technical documentation is needed.
            Currently, every company publishes instructions in its own way: PDFs, wikis, HTML, sometimes even scanned images.
            AI struggles to navigate this variety.
        </p>

        <p class="inter-small">
            The optimal solution is to create a centralized documentation repository, where:
        </p>

        <ul class="inter-small">
            <li>manufacturers upload official documents themselves,</li>
            <li>or a bot regularly collects them from websites (where the “Documentation” section is specially marked in the sitemap).</li>
        </ul><br>

        <p class="inter-small">
            This database could store documents in their original form and in a processed AI-friendly format, where the structure is standardized.
            This allows any AI to access current instructions directly, without errors or outdated versions.
        </p><br>
<img src="images/ddb2.jpg" alt="Central document database" style="width:100%; margin:10px 0;"><br>
        <p class="inter-medium">So that AI doesn’t slow down progress</p><br>

        <p class="inter-small">
            As long as AI relies on outdated data, it remains a tool from the past.
            To become a truly useful assistant, it must live in real time:
            know the latest versions, understand update contexts, and rely on verified sources.
        </p>

        <p class="inter-small">
            Creating a unified technical knowledge base is not just convenient.
            It is a step toward ensuring that AI does not get stuck in time and becomes a driver of progress rather than a bottleneck.
        
<br> <br>
Published: October 24, 2025

</p>

      </div>






<!-- Article 2 -->





<div class="card" id="university-lectures">
  <p class="inter-medium">University Lectures as a New Source for Safe AI Training</p><br>
  
<img src="images/dai.jpg" alt="University lecture and AI illustration" style="width:100%; margin:10px 0;"><br>

<p class="inter-small">
    Modern artificial intelligence models face a fundamental problem: a lack of high-quality, representative training data. Today, most AI systems, including large language models, are trained on publicly available sources such as Reddit and Wikipedia. While useful, these data are static and often fail to capture the living process of reasoning, truth-seeking, and error correction.
  </p>

  <p class="inter-small">
    Elon Musk recently emphasized in an interview that the focus is shifting toward synthetic data, specifically created for AI training. However, synthetic data cannot always replicate the real dynamics of human thinking, debates, and collective discovery of truth.
  </p> <br>

  <p class="inter-medium">Why Learning from Live Processes Matters</p><br>

  <p class="inter-small">
    Imagine equipping educational institutions with devices that record lectures, discussions, and debates between students and professors. These devices could capture not only speech but also visual materials like diagrams, blackboards, and presentations. This approach would allow AI models to learn from real interactions, where:
  </p>

  <ul class="inter-small">
    <li>mistakes are corrected during discussion,</li>
    <li>arguments and counterarguments are developed,</li>
    <li>collective understanding and truth emerge through debate.</li>
  </ul>

  <p class="inter-small">
    This is not just text — it is dynamic learning, where AI observes how humans think, reason, and refine conclusions.
  </p><br>

  <p class="inter-medium">A Question of Fundamental AI Safety</p><br>

  <p class="inter-small">
    This approach is directly related to foundational AI safety. The better AI training is structured, the lower the risk that errors, biases, or vulnerabilities will propagate to real-world systems.
  </p>

  <p class="inter-small">
    Our project, a collective red-teaming AI system, creates a network of AIs that monitor each other, detect errors, and identify potential threats. If models are trained on live discussions and real reasoning processes, the number of potential threats reaching global systems is significantly reduced.
  </p><br>

  <p class="inter-medium">Benefits of Learning from Live Data</p><br>

  <ul class="inter-small">
    <li>Richer knowledge base — AI learns from real reasoning, not just static text.</li>
    <li>Development of critical thinking — the ability to analyze different viewpoints and identify contradictions.</li>
    <li>Improved safety — errors and potential threats are detected early, before deployment in real systems.</li>
    <li>Innovative approach to education and AI — integrating AI into the learning process to improve teaching and analysis.</li>
  </ul>

  <p class="inter-medium">Conclusion</p><br>

  <p class="inter-small">
    Shifting from static training on Reddit and Wikipedia to live learning from lectures and debates is a key step toward creating safe and robust AI. Only by observing real human reasoning and debate can AI learn to understand, reason, and assess risks.
  </p>

  <p class="inter-small">
    The better foundational AI safety is established, the fewer threats will reach the level of global systems, such as our collective red-teaming project, and the safer the future of technology will be for humanity.
 <br> <br>
Published: October 21, 2025
</p>
</div>



    









      

    </div>
  </section>

 <footer style="text-align:center; padding:10px 0;">
  <p>2025 PROJECT: Global AI Security Initiative</p>
</footer>

</body>
</html>